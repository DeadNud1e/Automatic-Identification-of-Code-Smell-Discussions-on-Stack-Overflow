{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('posts_with_and_without_code_smell_discussions(6000).csv',lineterminator='\\n')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = df.iloc[:2000].sample(n=341)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross.to_csv('cross_val_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2000):\n",
    "    print(cross['post'].iloc[i])\n",
    "    print(cross['class'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.post[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['class'] ==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['class'] ==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset =\"post\", \n",
    "                     keep = False, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset='post').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.barplot(x = df['class'].value_counts().index, y=df['class'].value_counts(), data=df)\n",
    "labels = ['non code smell posts', 'code smell posts']\n",
    "label = [1204, 1101]\n",
    "for i in range(len(label)):\n",
    "    plt.text(x = df['class'].value_counts().index[i], \n",
    "             y =df['class'].value_counts()[i] + 10,  s = label[i])\n",
    "plt.xticks(range(2), labels)\n",
    "ax.set_ylabel('Amount of posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.post\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x , y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vect = vect.transform(X_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train_vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Amount of features: {}\".format(len(feature_names)))\n",
    "print(\"Some features:\\n{}\".format(feature_names[8900:9000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(), X_train_vect, y_train, cv=5)\n",
    "print(\"Cross val score: {:.6f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 20, 30]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train_vect, y_train)\n",
    "print(\"Best score: {:.6f}\".format(grid.best_score_))\n",
    "print(\"best params: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "X_test_vect = vect.transform(X_test)\n",
    "print(accuracy_score(y_test, grid.predict(X_test_vect)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, grid.predict(X_test_vect))))\n",
    "report = classification_report(y_test, grid.predict(X_test_vect), target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_look = X_test.iloc[430:440]\n",
    "X_look.iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_look = y_test.iloc[430:440]\n",
    "y_look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vect.transform(X_look)\n",
    "pred = grid.predict(X_vect)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.loc[0] ='I have a model that has several attributes that are optional when the model is being saved. I have several instance methods that use these attributes and perform calculations but I would like to check first if they are not nil as I will get the dreaded no method error nil nil class Apart from littering my code with .present? is there a better way of doing this? EDIT:Here is my code so fardef is_valid?   (has_expired? === false and has_uses_remaining?) ? true : falseenddef has_expired?   expiry_date.present? ? expiry_date.past? : falseenddef remaining_uses  if number_of_uses.present?    number_of_uses - uses_count  endenddef has_uses_remaining?  number_of_uses.present? ? (remaining_uses &gt; 0) : trueendI feel like dropping in .present? to perform checks has a bad , I have looked into the null object pattern but it doesnt seem to make sense here as the object is present but some of the attributes are nil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.loc[1] = ' If you have a statically allocated array code smell, the Visual Studio debugger can easily display all of the array elements.  However, if you have an array allocated dynamically and pointed to by a pointer, it will only display the first element of the array when you click the + to expand it.  Is there an easy way to tell the debugger, show me this data as an array of type Foo and size X? '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.loc[2] = 'How can I round a decimal to at least 2 decimal places and have it kept as a decimal? I know I can do this, but it has a code smell.var myResult = Decimal.Parse(myDecimal.ToString(\"0.00##\")); These are the expected results. 0.028 -> 0.028 0.02999 -> 0.03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vect = vect.transform(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr_pred = grid.predict(tr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "X_test_vect = vect.transform(X_test)\n",
    "y_pred = grid.predict(X_test_vect)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_vect, y_train)\n",
    "y_pred = rfc.predict(X_test_vect)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(\"Precision score: {}\".format(precision_score(y_test,y_pred)))\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Recall score: {}\".format(recall_score(y_test,y_pred)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "y_pred = rfc.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_metric.functions import BinaryClassification\n",
    "# Visualisation with plot_metric\n",
    "bc = BinaryClassification(y_test, y_pred, labels=[\"Class 1\", \"Class 2\"])\n",
    "\n",
    "# Figures\n",
    "plt.figure(figsize=(5,5))\n",
    "bc.plot_roc_curve()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_estimators': range(10,500, 50)}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train_vect, y_train)\n",
    "print(\"Best score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"best params: \", grid.best_params_)\n",
    "y_pred = grid.predict(X_test_vect)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier()\n",
    "knc.fit(X_train_vect, y_train)\n",
    "y_pred = knc.predict(X_test_vect)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Precision score: {}\".format(precision_score(y_test,y_pred)))\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Recall score: {}\".format(recall_score(y_test,y_pred)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "y_pred = knc.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation with plot_metric\n",
    "bc = BinaryClassification(y_test, y_pred, labels=[\"Class 1\", \"Class 2\"])\n",
    "\n",
    "# Figures\n",
    "plt.figure(figsize=(5,5))\n",
    "bc.plot_roc_curve()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': range(1,20)}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train_vect, y_train)\n",
    "print(\"Best score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"best params: \", grid.best_params_)\n",
    "y_pred = grid.predict(X_test_vect)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train_vect, y_train)\n",
    "y_pred = svc.predict(X_test_vect)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Precision score: {}\".format(precision_score(y_test,y_pred)))\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Recall score: {}\".format(recall_score(y_test,y_pred)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "y_pred = svc.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualisation with plot_metric\n",
    "bc = BinaryClassification(y_test, y_pred, labels=[\"Class 1\", \"Class 2\"])\n",
    "\n",
    "# Figures\n",
    "plt.figure(figsize=(5,5))\n",
    "bc.plot_roc_curve()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "gpc = GaussianProcessClassifier()\n",
    "gpc.fit(X_train_vect.toarray(), y_train)\n",
    "y_pred = gpc.predict(X_test_vect.toarray())\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision score: {}\".format(precision_score(y_test,y_pred)))\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Recall score: {}\".format(recall_score(y_test,y_pred)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "y_pred = gpc.predict(X_test_vect.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation with plot_metric\n",
    "bc = BinaryClassification(y_test, y_pred, labels=[\"Class 1\", \"Class 2\"])\n",
    "\n",
    "# Figures\n",
    "plt.figure(figsize=(5,5))\n",
    "bc.plot_roc_curve()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_importances_\n",
    "np.argsort(rfc.feature_importances_)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[14837]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "log =LogisticRegression()\n",
    "log.fit(X_train_vect, y_train)\n",
    "y_pred = log.predict(X_test_vect)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)\n",
    "print(f1_score(y_test, y_pred))\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision score: {}\".format(precision_score(y_test,y_pred)))\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Recall score: {}\".format(recall_score(y_test,y_pred)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "y_pred = log.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation with plot_metric\n",
    "bc = BinaryClassification(y_test, y_pred, labels=[\"Class 1\", \"Class 2\"])\n",
    "\n",
    "# Figures\n",
    "plt.figure(figsize=(5,5))\n",
    "bc.plot_roc_curve()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import seaborn as sns\n",
    "y_pred_prob1 = rfc.predict_proba(X_test_vect)[:,1]\n",
    "fpr1 , tpr1, thresholds1 = roc_curve(y_test, y_pred_prob1)\n",
    "\n",
    "y_pred_prob2 = knc.predict_proba(X_test_vect)[:,1]\n",
    "fpr2 , tpr2, thresholds2 = roc_curve(y_test, y_pred_prob2)\n",
    "\n",
    "y_pred_prob3 = svc.predict(X_test_vect)\n",
    "fpr3 , tpr3, thresholds3 = roc_curve(y_test, y_pred_prob3)\n",
    "\n",
    "y_pred_prob4 = gpc.predict_proba(X_test_vect.toarray())[:,1]\n",
    "fpr4 , tpr4, thresholds4 = roc_curve(y_test, y_pred_prob4)\n",
    "\n",
    "y_pred_prob5 = log.predict_proba(X_test_vect)[:,1]\n",
    "fpr5 , tpr5, thresholds5 = roc_curve(y_test, y_pred_prob5)\n",
    "\n",
    "\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.plot(fpr1, tpr1, label= \"RandomForest\")\n",
    "plt.plot(fpr2, tpr2, label= \"K-Nearest Neighbors\")\n",
    "plt.plot(fpr3, tpr3, label= \"Support Vector Machine\")\n",
    "plt.plot(fpr4, tpr4, label= \"Gaussian Process\")\n",
    "plt.plot(fpr5, tpr5, label= \"Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.axis('on')\n",
    "#plt.box(False)\n",
    "#plt.title('Receiver Operating Characteristic')\n",
    "plt.savefig('roc.svg', bbox_inches=0, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_pred_prob1 = rfc.predict_proba(X_test_vect)[:,1]\n",
    "fpr1 , tpr1, thresholds1 = precision_recall_curve(y_test, y_pred_prob1)\n",
    "\n",
    "y_pred_prob2 = knc.predict_proba(X_test_vect)[:,1]\n",
    "fpr2 , tpr2, thresholds2 = precision_recall_curve(y_test, y_pred_prob2)\n",
    "\n",
    "y_pred_prob3 = svc.predict(X_test_vect)\n",
    "fpr3 , tpr3, thresholds3 = precision_recall_curve(y_test, y_pred_prob3)\n",
    "\n",
    "y_pred_prob4 = gpc.predict_proba(X_test_vect.toarray())[:,1]\n",
    "fpr4 , tpr4, thresholds4 = precision_recall_curve(y_test, y_pred_prob4)\n",
    "\n",
    "y_pred_prob5 = log.predict_proba(X_test_vect)[:,1]\n",
    "fpr5 , tpr5, thresholds5 = precision_recall_curve(y_test, y_pred_prob5)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(fpr1, tpr1, label= \"RandomForest\")\n",
    "plt.plot(fpr2, tpr2, label= \"K-Nearest Neighbors\")\n",
    "plt.plot(fpr3, tpr3, label= \"Support Vector Machine\")\n",
    "plt.plot(fpr4, tpr4, label= \"Gaussian Process\")\n",
    "plt.plot(fpr5, tpr5, label= \"Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.box(False)\n",
    "#plt.title('Precision-Recall Curve')\n",
    "plt.savefig('prc.svg', bbox_inches=0, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.classifiers import plot_precision_recall_curve_with_cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clf = LogisticRegression()\n",
    "plot_precision_recall_curve_with_cv(clf, X_test_vect, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr_pred = log.predict(tr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0.001, 0.01, 0.1, 1, 10, 20, 30,]:\n",
    "    log =LogisticRegression(C=i)\n",
    "    log.fit(X_train_vect, y_train)\n",
    "    y_pred = log.predict(X_test_vect)\n",
    "    print(i, '-F-score-',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [10, 40, 60, 100, 150, 200, 300, 350, 400, 450]:\n",
    "    log =RandomForestClassifier(n_estimators=i)\n",
    "    log.fit(X_train_vect, y_train)\n",
    "    y_pred = log.predict(X_test_vect)\n",
    "    print(i, '-F-score-',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "y_pred = log.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr_pred = grid.predict(tr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = log.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectors.reshape(18168, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve = pd.DataFrame(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[563]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.linalg.norm(log.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "#wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "#wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg = logreg.fit(X_train_word_average, train['class'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test['class']))\n",
    "print(classification_report(test['class'], y_pred,target_names=['Non-smell', 'Smell']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = RandomForestClassifier(n_estimators=100)\n",
    "logreg = logreg.fit(X_train_word_average, train['class'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test['class']))\n",
    "print(classification_report(test['class'], y_pred,target_names=['Non-smell', 'Smell']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = KNeighborsClassifier()\n",
    "logreg = logreg.fit(X_train_word_average, train['class'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test['class']))\n",
    "print(classification_report(test['class'], y_pred,target_names=['Non-smell', 'Smell']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = GaussianProcessClassifier()\n",
    "logreg = logreg.fit(X_train_word_average, train['class'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test['class']))\n",
    "print(classification_report(test['class'], y_pred,target_names=['Non-smell', 'Smell']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = SVC()\n",
    "logreg = logreg.fit(X_train_word_average, train['class'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test['class']))\n",
    "print(classification_report(test['class'], y_pred,target_names=['Non-smell', 'Smell']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df['class'], random_state=42, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['post']), tags=[r['class']]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['post']), tags=[r['class']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged.values[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "tokens = [nl.word_tokenize(sentences) for sentences in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = gensim.models.Word2Vec(size=300,window=10,min_count=2,iter=10)\n",
    "model.build_vocab(tokens)\n",
    "#model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(tokens, total_examples=len(tokens), epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial').fit(model.wv.syn0, y_train)\n",
    "\n",
    "# Prediction of the first 15 samples of all features\n",
    "predict = clf.predict(model.wv.syn0[:15, :])\n",
    "# Calculating the score of the predictions\n",
    "score = clf.score(model.wv.syn0, Y_train)\n",
    "#print(\"\\nPrediction word2vec : \\n\", predict)\n",
    "print(\"Score word2vec : \\n\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Word2Vec(size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['post'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_tagged = train.apply( lambda r: TaggedDocument(words=tokenize_text(r['post']), tags=[r['class']]), axis=1)\n",
    "test_tagged = test.apply( lambda r: TaggedDocument(words=tokenize_text(r['post']), tags=[r['class']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "def label_sentences(corpus, label_type):\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df['class'], random_state=42, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow =FastText(size=4, window=3, min_count=1)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn import utils\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext():\n",
    "    \n",
    "    print('loading word embeddings...')\n",
    "    embeddings_index = {}\n",
    "    f = open('wiki-news-300d-1M.vec',encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.strip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index=load_fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_matrix(embedding_dict, emb_size=300):\n",
    "    num_words = len(word_index)\n",
    "    embedding_matrix = np.zeros((num_words, emb_size))\n",
    "\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i > num_words:\n",
    "            continue\n",
    "\n",
    "    emb_vec = embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def new_model(embedding_matrix):\n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "\n",
    "    x = Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(X_train)\n",
    "sequences=tokenizer_obj.texts_to_sequences(X_train)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,\n",
    "                        maxlen=MAX_LEN,\n",
    "                        truncating='post',\n",
    "                        padding='post')\n",
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))\n",
    "num_words = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index=embeddings_index\n",
    "embedding_matrix=prepare_matrix(embeddings_index)\n",
    "model=new_model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['post'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['post'])\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 300, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.3))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(50))\n",
    "model_glove.add(Dense(1, activation='softmax'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.fit(data, np.array(df['class']), validation_split=0.4, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df['class'], random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(size=4, window=3, min_count=1)\n",
    "model.build_vocab(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(sentences=X_train, total_examples=len(X_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [model.wv.get_vector(doc) for doc in X_train]\n",
    "test = [model.wv.get_vector(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.wv.get_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train, X_train = vec_for_learning(model, train_tagged)\n",
    "#y_test, X_test = vec_for_learning(model, test_tagged)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(regressors, y_train)\n",
    "y_pred = logreg.predict(test)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(\"Precision score: {}\".format(precision_score(y_test,y_pred)))\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Recall score: {}\".format(recall_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['post'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['post'])\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(BatchNormalization())\n",
    "model_glove.add(Dropout(0.3))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(50))\n",
    "model_glove.add(Dense(1, activation='softmax'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.fit(data, np.array(df['class']), validation_split=0.4, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(X_train)\n",
    "X_train_vect = vect.transform(X_train)\n",
    "print(\"X_train с min_df: {}\".format(repr(X_train_vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train_vect, y_train)\n",
    "print(\"Cross val score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"every 10th stop word:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words='english').fit(X_train)\n",
    "X_train_vect = vect.transform(X_train)\n",
    "print(\"X_train с min_df: {}\".format(repr(X_train_vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train_vect, y_train)\n",
    "print(\"Cross val score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n",
    "LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Cross val score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0.001, 0.01, 0.1, 1, 10, 20, 30,]:\n",
    "    log =LogisticRegression(C=i)\n",
    "    log.fit(X_train, y_train)\n",
    "    y_pred = log.predict(X_test)\n",
    "    print(i, '-F-score-',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "X_train_vect = vectorizer.transform(X_train)\n",
    "max_value = X_train_vect.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(\"lowest features tfidf:\\n{}\".format(feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"important features tfidf: \\n{}\".format(feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"features idf:\\n{}\".format(\n",
    "feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 20, 30],\n",
    "\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Cross val score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for t in [(1, 1), (1, 2), (1, 3)]:\n",
    "    for i in [0.001, 0.01, 0.1, 1, 10, 20, 30,]:\n",
    "        pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None, ngram_range=t),\n",
    "LogisticRegression(C=i))\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        print(t,'-ngram, C-',i, '-F-score-',f1_score(y_test, y_pred))\n",
    "        report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-smell', 'Smell'],  digits=3)\n",
    "print(report)\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(\"Precision score: {}\".format(precision_score(y_test,y_pred)))\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Recall score: {}\".format(recall_score(y_test,y_pred)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for t in [(1, 1), (1, 2), (1, 3)]:\n",
    "    for i in [10, 50, 100, 150, 200, 300, 350]:\n",
    "        pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None, ngram_range=t),\n",
    "RandomForestClassifier(n_estimators=i))\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        print(t,'-ngram, C-',i, '-F-score-',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect = vectorizer.transform(X_test)\n",
    "grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score: {}'.format(f1_score(y_test, grid.predict(X_test))))\n",
    "report = classification_report(y_test, grid.predict(X_test), target_names=['Non-smell', 'Smell'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allscores=grid.cv_results_['mean_test_score']\n",
    "print(allscores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
